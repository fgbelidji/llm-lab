# llm-lab

Practical recipes for running LLM inference on cloud GPU infrastructure.

## Projects

| Project | Description |
|---------|-------------|
| [batch-ocr-inference/](batch-ocr-inference/) | 3-stage OCR pipeline with DeepSeek-OCR on HF Jobs, SageMaker, and Cloud Run |
| [aws/](aws/) | AWS SageMaker inference examples |
