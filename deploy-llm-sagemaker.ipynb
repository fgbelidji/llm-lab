{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88894761",
   "metadata": {},
   "source": [
    "### [Deploy LLM on Amazon SageMaker](https://huggingface.co/docs/sagemaker/en/inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b99af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install  sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d431ee",
   "metadata": {},
   "source": [
    "### Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cda7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='sagemaker-dlc-demo')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3a4c376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::754289655784:role/sagemaker-dlc-demo'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41853a14",
   "metadata": {},
   "source": [
    "### Select image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82efc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.7.0-tgi3.3.6-gpu-py311-cu124-ubuntu22.04-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130052d",
   "metadata": {},
   "source": [
    "### Configure instance and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d666beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g6.16xlarge\"\n",
    "health_check_timeout = 900\n",
    "\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"Qwen/Qwen3-4B-Thinking-2507\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': \"1\", # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"2048\",  # Max length of input text\n",
    "  #'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\"\n",
    "}\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config,\n",
    "  name=\"qwen3-4b-thinking-2507-demo-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ba76c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFaceModel at 0x135a89950>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa74a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: qwen3-4b-thinking-2507-demo-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19810521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"user\", \"content\": \"Give me a short introduction to large language model\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 128,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c57b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user asked for a short introduction to large language models. Let me start by recalling what I know. LLMs are a big topic in AI, so I need to keep it concise but informative. They want it short, so I shouldn't go into too much detail.\n",
      "\n",
      "First, I should define what an LLM is. Maybe mention they're AI models trained on massive text data. Highlight key features like understanding and generating human-like text. The user might be a beginner, so avoid jargon where possible. Terms like \"neural networks\" are okay but should be explained simply.\n",
      "\n",
      "I remember that LLMs like\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d99ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
