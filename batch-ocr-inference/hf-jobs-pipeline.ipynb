{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek OCR Job Pipeline\n",
    "\n",
    "This notebook submits a three-stage pipeline to Hugging Face Jobs using the `ds-batch-ocr.py` driver:\n",
    "\n",
    "1. **extract** – run DeepSeek OCR over a dataset batch, save Markdown, and crop detected figures.\n",
    "2. **describe** – generate captions for the extracted figure crops.\n",
    "3. **assemble** – combine documents and figure descriptions into polished Markdown bundles.\n",
    "\n",
    "Each stage runs as an independent Job so you can mix hardware, retry specific steps, or parallelize work. The helper utilities below keep track of job IDs and share artifacts between stages via the Jobs repositories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from huggingface_hub import fetch_job_logs, inspect_job, run_uv_job, whoami\n",
    "from huggingface_hub._jobs_api import JobInfo, JobStage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Pipeline configuration ----\n",
    "HUB_IMAGE = \"vllm/vllm-openai:nightly-758ea2e980a1eeacec6097bfd98bd0a7c8fb864a\"\n",
    "HARDWARE_FLAVOR = \"a100-large\"\n",
    "JOB_TIMEOUT = \"3h\"\n",
    "\n",
    "PIPELINE_ROOT = \"/workspace/pipeline\"\n",
    "STAGE1_DIR = f\"{PIPELINE_ROOT}/stage1\"\n",
    "STAGE2_DIR = f\"{PIPELINE_ROOT}/stage2\"\n",
    "STAGE3_DIR = f\"{PIPELINE_ROOT}/stage3\"\n",
    "\n",
    "USERNAME = whoami()[\"name\"]\n",
    "\n",
    "CODE_REPO_ID = f\"{USERNAME}/deepseek-ocr-job-code-final\"\n",
    "CODE_REPO_TYPE = \"dataset\"\n",
    "CODE_REPO_REVISION = \"main\"\n",
    "CODE_LOCAL_DIR = \"/tmp/deepseek-ocr-job-code\"\n",
    "\n",
    "BASE_ENV: Dict[str, str] = {\n",
    "    \"MODEL_ID\": \"deepseek-ai/DeepSeek-OCR\",\n",
    "    \"SERVED_MODEL_NAME\": \"deepseek-ocr\",\n",
    "    \"HOST\": \"0.0.0.0\",\n",
    "    \"PORT\": \"8000\",\n",
    "    \"TENSOR_PARALLEL_SIZE\": \"1\",\n",
    "    \"MAX_MODEL_LEN\": \"8192\",\n",
    "    \"GPU_MEMORY_UTILIZATION\": \"0.90\",\n",
    "    \"JOB_CODE_REPO\": CODE_REPO_ID,\n",
    "    \"JOB_CODE_REVISION\": CODE_REPO_REVISION,\n",
    "    \"JOB_CODE_LOCAL_DIR\": CODE_LOCAL_DIR,\n",
    "    \"PIPELINE_BATCH_SIZE\": \"4\",\n",
    "    \"PIPELINE_MAX_CONCURRENCY\": \"4\",\n",
    "    \"PIPELINE_TOKEN_MARGIN\": \"512\",\n",
    "    \"PIPELINE_ARTIFACT_STRATEGY\": \"hf-hub\",\n",
    "    \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\", \"\"),\n",
    "}\n",
    "\n",
    "DATASET_ENV: Dict[str, str] = {\n",
    "    \"DATASET_NAME\": \"HuggingFaceM4/FineVision\",\n",
    "    \"DATASET_CONFIG\": \"olmOCR-mix-0225-documents\",\n",
    "    \"DATASET_SPLIT\": \"train\",\n",
    "    \"MAX_SAMPLES\": \"10\",\n",
    "}\n",
    "\n",
    "PROMPT_ENV: Dict[str, str] = {\n",
    "    \"DOC_PROMPT\": \"<image>\\n<|grounding|>Convert this document to Markdown.\",\n",
    "    \"DOC_MAX_TOKENS\": \"4096\",\n",
    "    \"DOC_TEMPERATURE\": \"0.4\",\n",
    "    \"FIGURE_PROMPT\": \"<image>\\nDescribe this image in detail. If it's a table or a graph, don't parse it, just describe it in simple words or return  `graph`\",\n",
    "    \"FIGURE_MAX_TOKENS\": \"512\",\n",
    "    \"FIGURE_TEMPERATURE\": \"0.\",\n",
    "}\n",
    "\n",
    "ARTIFACT_REPO_ID = f\"{USERNAME}/deepseek-ocr-artifacts-test-Z\"\n",
    "ARTIFACT_REPO_BRANCH = None\n",
    "STAGE1_UPLOAD_REPO = ARTIFACT_REPO_ID\n",
    "STAGE1_UPLOAD_PATH_IN_REPO = \"stage1\"\n",
    "STAGE1_UPLOAD_BRANCH = ARTIFACT_REPO_BRANCH\n",
    "STAGE1_UPLOAD_COMMIT_MESSAGE = None\n",
    "\n",
    "STAGE2_UPLOAD_REPO = ARTIFACT_REPO_ID\n",
    "STAGE2_UPLOAD_PATH_IN_REPO = \"stage2\"\n",
    "STAGE2_UPLOAD_BRANCH = ARTIFACT_REPO_BRANCH\n",
    "STAGE2_UPLOAD_COMMIT_MESSAGE = None\n",
    "\n",
    "ASSEMBLED_DATASET_REPO = ARTIFACT_REPO_ID\n",
    "ASSEMBLED_DATASET_PATH_IN_REPO = \"stage3\"\n",
    "ASSEMBLED_DATASET_COMMIT_MESSAGE = \"Add assembled DeepSeek OCR batch\"\n",
    "ASSEMBLED_DATASET_BRANCH = ARTIFACT_REPO_BRANCH\n",
    "\n",
    "USERNAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "CODE_PATHS = [\n",
    "    Path(\"/home/ec2-user/aws-llm-lab/ds-batch-ocr.py\"),\n",
    "    Path(\"/home/ec2-user/aws-llm-lab/hf_job_runner.py\"),\n",
    "    Path(\"/home/ec2-user/aws-llm-lab/ds_batch_ocr\"),\n",
    "]\n",
    "\n",
    "api = HfApi()\n",
    "create_repo(repo_id=CODE_REPO_ID, repo_type=CODE_REPO_TYPE, exist_ok=True)\n",
    "create_repo(repo_id=ARTIFACT_REPO_ID, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "bundle_dir = Path(tempfile.mkdtemp(prefix=\"job-code-\"))\n",
    "for path in CODE_PATHS:\n",
    "    target = bundle_dir / path.name\n",
    "    if path.is_dir():\n",
    "        shutil.copytree(path, target, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(path, target)\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=str(bundle_dir),\n",
    "    repo_id=CODE_REPO_ID,\n",
    "    repo_type=CODE_REPO_TYPE,\n",
    "    commit_message=\"Sync DeepSeek OCR HF job code\",\n",
    ")\n",
    "\n",
    "print(f\"Uploaded code to {CODE_REPO_ID} ({CODE_REPO_TYPE})\")\n",
    "bundle_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = Path(\"/home/ec2-user/aws-llm-lab/hf_job_runner.py\")\n",
    "if not SCRIPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Job runner script not found at {SCRIPT_PATH}\")\n",
    "\n",
    "CODE_RUN_URL = (\n",
    "    f\"https://huggingface.co/datasets/{CODE_REPO_ID}/resolve/\"\n",
    "    f\"{CODE_REPO_REVISION or 'main'}/{SCRIPT_PATH.name}\"\n",
    ")\n",
    "CODE_RUN_URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_env(*layers: Optional[Dict[str, str]]) -> Dict[str, str]:\n",
    "    env: Dict[str, str] = {}\n",
    "    for layer in layers:\n",
    "        if not layer:\n",
    "            continue\n",
    "        for key, value in layer.items():\n",
    "            if value is None:\n",
    "                continue\n",
    "            env[key] = str(value)\n",
    "    return env\n",
    "\n",
    "\n",
    "def launch_stage(stage: str, env_overrides: Optional[Dict[str, str]] = None) -> JobInfo:\n",
    "    env = merged_env(BASE_ENV, PROMPT_ENV, env_overrides)\n",
    "    env[\"PIPELINE_STAGE\"] = stage\n",
    "\n",
    "    job = run_uv_job(\n",
    "        CODE_RUN_URL,\n",
    "        image=HUB_IMAGE,\n",
    "        flavor=HARDWARE_FLAVOR,\n",
    "        env=env,\n",
    "        timeout=JOB_TIMEOUT,\n",
    "    )\n",
    "    print(f\"Submitted stage '{stage}' job: {job.url}\")\n",
    "    return job\n",
    "\n",
    "\n",
    "def wait_for_completion(job: JobInfo, poll_interval: int = 60) -> JobInfo:\n",
    "    print(f\"Monitoring {job.url}\")\n",
    "    while True:\n",
    "        print(job.id)\n",
    "        info = inspect_job(job_id=job.id)\n",
    "        stage_value = info.status.stage\n",
    "        print(f\"  -> {stage_value}\", flush=True)\n",
    "        if stage_value not in {JobStage.RUNNING, \"RUNNING\", \"UPDATING\"}:\n",
    "            print(f\"Job finished in state: {stage_value}\")\n",
    "            return info\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "\n",
    "def job_repo_id(job: JobInfo) -> str:\n",
    "    return f\"jobs/{job.owner.name}/{job.id}\"\n",
    "\n",
    "\n",
    "def stream_job_logs(job: JobInfo, tail: Optional[int] = None) -> None:\n",
    "    lines = list(fetch_job_logs(job_id=job.id, namespace=job.owner.name))\n",
    "    if tail is not None:\n",
    "        lines = lines[-tail:]\n",
    "    for line in lines:\n",
    "        print(line, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_env = merged_env(\n",
    "    DATASET_ENV,\n",
    "    {\n",
    "        \"STAGE1_OUTPUT_DIR\": STAGE1_DIR,\n",
    "        \"EXTRACT_BATCH_SIZE\": \"64\",\n",
    "        \"EXTRACT_MAX_CONCURRENCY\": \"4\",\n",
    "        \"STAGE1_UPLOAD_REPO\": STAGE1_UPLOAD_REPO,\n",
    "        \"STAGE1_UPLOAD_PATH_IN_REPO\": STAGE1_UPLOAD_PATH_IN_REPO,\n",
    "        \"STAGE1_UPLOAD_BRANCH\": STAGE1_UPLOAD_BRANCH,\n",
    "    },\n",
    ")\n",
    "\n",
    "stage2_base_env = {\n",
    "    \"STAGE1_DIR\": STAGE1_DIR,\n",
    "    \"STAGE2_OUTPUT_DIR\": STAGE2_DIR,\n",
    "    \"DESCRIBE_BATCH_SIZE\": \"8\",\n",
    "    \"DESCRIBE_MAX_CONCURRENCY\": \"4\",\n",
    "    \"STAGE2_UPLOAD_PATH_IN_REPO\": STAGE2_UPLOAD_PATH_IN_REPO,\n",
    "    \"STAGE2_UPLOAD_BRANCH\": STAGE2_UPLOAD_BRANCH,\n",
    "}\n",
    "\n",
    "stage3_base_env = {\n",
    "    \"STAGE1_DIR\": STAGE1_DIR,\n",
    "    \"STAGE2_DIR\": STAGE2_DIR,\n",
    "    \"STAGE3_OUTPUT_DIR\": STAGE3_DIR,\n",
    "}\n",
    "\n",
    "stage1_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_job = launch_stage(\"extract\", stage1_env)\n",
    "stage1_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_info = wait_for_completion(stage1_job)\n",
    "stage1_repo = STAGE1_UPLOAD_REPO or job_repo_id(stage1_info)\n",
    "stage1_job_id = stage1_info.id\n",
    "stage1_job_owner = stage1_info.owner.name\n",
    "print(\"Stage 1 artifacts repo:\", stage1_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: inspect the tail of the job logs\n",
    "stream_job_logs(stage1_info, tail=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_repo = \"florentgbelidji/deepseek-ocr-artifacts-test-XX\"\n",
    "stage2_repo = \"florentgbelidji/deepseek-ocr-artifacts-test-XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_env = merged_env(\n",
    "    stage2_base_env,\n",
    "    {\n",
    "        \"STAGE1_REPO_ID\": stage1_repo,\n",
    "        \"STAGE1_ARTIFACT_STRATEGY\": \"hf-hub\",\n",
    "        \"STAGE1_MANIFEST_NAME\": f\"{STAGE1_UPLOAD_PATH_IN_REPO}/manifest.json\",\n",
    "    },\n",
    ")\n",
    "\n",
    "stage2_job = launch_stage(\"describe\", stage2_env)\n",
    "stage2_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_info = wait_for_completion(stage2_job)\n",
    "stage2_repo = STAGE2_UPLOAD_REPO or job_repo_id(stage2_info)\n",
    "stage2_job_id = stage2_info.id\n",
    "stage2_job_owner = stage2_info.owner.name\n",
    "print(\"Stage 2 artifacts repo:\", stage2_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: inspect the second-stage logs\n",
    "stream_job_logs(stage2_info, tail=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_overrides = {\n",
    "    \"STAGE1_REPO_ID\": stage1_repo,\n",
    "    \"STAGE1_ARTIFACT_STRATEGY\": \"hf-hub\",\n",
    "    \"STAGE1_MANIFEST_NAME\": f\"{STAGE1_UPLOAD_PATH_IN_REPO}/manifest.json\",\n",
    "    \"STAGE2_REPO_ID\": stage2_repo,\n",
    "    \"STAGE2_ARTIFACT_STRATEGY\": \"hf-hub\",\n",
    "    \"STAGE2_MANIFEST_NAME\": f\"{STAGE2_UPLOAD_PATH_IN_REPO}/figure_descriptions.json\",\n",
    "    \"ASSEMBLED_DATASET_REPO\": ASSEMBLED_DATASET_REPO,\n",
    "    \"ASSEMBLED_DATASET_PATH_IN_REPO\": ASSEMBLED_DATASET_PATH_IN_REPO,\n",
    "    \"ASSEMBLED_DATASET_COMMIT_MESSAGE\": ASSEMBLED_DATASET_COMMIT_MESSAGE,\n",
    "    \"ASSEMBLED_DATASET_BRANCH\": ASSEMBLED_DATASET_BRANCH,\n",
    "}\n",
    "\n",
    "stage3_env = merged_env(stage3_base_env, stage3_overrides)\n",
    "\n",
    "stage3_job = launch_stage(\"assemble\", stage3_env)\n",
    "stage3_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_info = wait_for_completion(stage3_job)\n",
    "final_repo = ASSEMBLED_DATASET_REPO or job_repo_id(stage3_info)\n",
    "print(\"Final bundle repo:\", final_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: inspect the third-stage logs\n",
    "stream_job_logs(stage3_info, tail=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"stage1_repo\": stage1_repo, \"stage2_repo\": stage2_repo, \"stage3_repo\": final_repo}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
