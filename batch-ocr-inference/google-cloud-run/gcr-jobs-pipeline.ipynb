{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek OCR Pipeline - Google Cloud Run GPU\n",
    "\n",
    "This notebook runs a three-stage OCR pipeline on Google Cloud Run with GPU support:\n",
    "\n",
    "1. **Extract** ‚Äì Run DeepSeek OCR over a dataset, save Markdown and crop detected figures\n",
    "2. **Describe** ‚Äì Generate captions for extracted figures  \n",
    "3. **Assemble** ‚Äì Enrich Markdown with figure captions\n",
    "\n",
    "All three stages write a **Hugging Face dataset** to GCS. Each stage pulls the current dataset, adds new columns with its outputs, and saves the updated dataset back ‚Äî so by the end you have one dataset with all intermediate and final results.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Google Cloud credentials configured\n",
    "- Cloud Run GPU enabled in your project\n",
    "- Required packages: `google-cloud-run`, `google-cloud-storage`\n",
    "\n",
    "**References:**\n",
    "- [Cloud Run GPU Documentation](https://cloud.google.com/run/docs/configuring/services/gpu)\n",
    "- [Cloud Run Pricing](https://cloud.google.com/run/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìë Table of Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "   - [Google Cloud Run](#‚òÅÔ∏è-Google-Cloud-Run)\n",
    "   - [The dataset](#üìä-The-dataset)\n",
    "   - [Inference Backend: vLLM](#‚ö°-Inference-Backend:-vLLM)\n",
    "2. [Authentication](#üîê-Authentication)\n",
    "3. [Configuration](#Configuration)\n",
    "4. [Create GCS Bucket](#Create-GCS-Bucket)\n",
    "5. [Build Container Image](#Build-Container-Image)\n",
    "6. [Helper Functions](#Helper-Functions-(Python-SDK))\n",
    "7. [Stage 1: Extract](#Stage-1:-Extract)\n",
    "8. [Stage 2: Describe](#Stage-2:-Describe)\n",
    "9. [Stage 3: Assemble](#Stage-3:-Assemble)\n",
    "10. [Load Final Dataset](#Load-Final-Dataset)\n",
    "11. [Cleanup](#Cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚òÅÔ∏è Google Cloud Run\n",
    "\n",
    "[Cloud Run](https://cloud.google.com/run) is a fully managed serverless platform that automatically scales containers. With [GPU support](https://cloud.google.com/run/docs/configuring/services/gpu), you can run ML inference workloads without managing infrastructure.\n",
    "\n",
    "**Key features for this pipeline:**\n",
    "- **L4 GPUs** ‚Äî 24GB VRAM, good for 7B parameter models\n",
    "- **Pay per use** ‚Äî billed per second while the job runs\n",
    "- **Auto-scaling** ‚Äî scales to zero when idle\n",
    "- **Container-based** ‚Äî package your code + dependencies in a Docker image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ How the pipeline code is shipped\n",
    "\n",
    "For every Cloud Run Job we launch, the logic is similar:\n",
    "\n",
    "**From this notebook**, we build a Docker image containing:\n",
    "1. The entrypoint script (`gcr_job_runner.py`)\n",
    "2. The pipeline code in `llm_ocr/`\n",
    "\n",
    "The image is pushed to Google Artifact Registry.\n",
    "\n",
    "**Then we launch a Cloud Run Job** that pulls the Docker image and runs it. The job:\n",
    "1. Starts a vLLM server with DeepSeek-OCR model\n",
    "2. Imports `llm_ocr.cli` and calls `main()` to run the requested pipeline stage\n",
    "3. Saves results to GCS (Google Cloud Storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö The dataset\n",
    "\n",
    "This pipeline uses **FineVision** (`HuggingFaceM4/FineVision`) as a large, mixed **image+text** corpus. FineVision aggregates many public sub-datasets into one unified interface.\n",
    "\n",
    "- **Dataset**: [`HuggingFaceM4/FineVision`](https://huggingface.co/datasets/HuggingFaceM4/FineVision)\n",
    "\n",
    "#### The `olmOCR` subsets\n",
    "\n",
    "The [`olmOCR-mix-0225`](https://arxiv.org/pdf/2502.18443) dataset from Allen AI contains **260,000 crawled PDF pages** from over 100,000 diverse PDFs ‚Äî academic papers, legal documents, public domain books, brochures, and more. It includes challenging content: graphics, handwritten text, multi-column layouts, tables, equations, and poor quality scans.\n",
    "\n",
    "Available configs:\n",
    "- `olmOCR-mix-0225-documents` ‚Äî general documents\n",
    "- `olmOCR-mix-0225-books` ‚Äî book pages\n",
    "\n",
    "> üìÑ **Note**: In this pipeline, **one document = one page** of a PDF.\n",
    "\n",
    "These mirror real-world enterprise use cases: contracts, invoices, reports, forms, and scanned documents that organizations need to digitize and extract structured information from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Inference Backend: vLLM\n",
    "\n",
    "This pipeline uses [**vLLM**](https://github.com/vllm-project/vllm) as the inference backend for DeepSeek-OCR. vLLM provides:\n",
    "\n",
    "- **High throughput** via continuous batching and PagedAttention\n",
    "- **OpenAI-compatible API** ‚Äî easy to integrate with existing code\n",
    "- **Efficient memory management** ‚Äî run large models on limited GPU memory\n",
    "\n",
    "The Cloud Run job uses the [`vllm/vllm-openai:latest`](https://hub.docker.com/r/vllm/vllm-openai) Docker image as the base. The pipeline sends batched requests (32 concurrent) to maximize throughput on L4 GPUs (~50 docs/min).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù DeepSeek-OCR Prompts\n",
    "\n",
    "DeepSeek-OCR supports different prompts for various OCR tasks. See the [official config.py](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py) for examples:\n",
    "\n",
    "| Use Case | Prompt |\n",
    "|----------|--------|\n",
    "| **Document ‚Üí Markdown** | `<image>\\n<\\|grounding\\|>Convert the document to markdown.` |\n",
    "| **General OCR** | `<image>\\n<\\|grounding\\|>OCR this image.` |\n",
    "| **Free OCR (no layout)** | `<image>\\nFree OCR.` |\n",
    "| **Parse figures** | `<image>\\nParse the figure.` |\n",
    "| **Describe image** | `<image>\\nDescribe this image in detail.` |\n",
    "\n",
    "We configure these prompts via environment variables `DOC_PROMPT` and `FIGURE_PROMPT` in our job configuration, re-using the special tokens from the [official DeepSeek-OCR config](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-run google-cloud-storage google-cloud-build google-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from google.cloud import run_v2\n",
    "from google.cloud import storage\n",
    "from google.protobuf import duration_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê Authentication\n",
    "\n",
    "Authenticate to Google Cloud before running jobs. You can use:\n",
    "- **Application Default Credentials (ADC)** ‚Äî recommended for local development\n",
    "- **Service account key** ‚Äî for production or CI/CD\n",
    "\n",
    "See:\n",
    "- [Authentication methods](https://docs.cloud.google.com/docs/authentication)\n",
    "- [Install the Google Cloud SDK](https://cloud.google.com/sdk/docs/install-sdk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated with project: huggingface-ml\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Set credentials from a service account key file\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/service-account-key.json\"\n",
    "\n",
    "# Option 2: If running locally, run this in terminal first:\n",
    "# gcloud auth application-default login\n",
    "\n",
    "# Verify credentials\n",
    "from google.auth import default\n",
    "credentials, project = default()\n",
    "print(f\"Authenticated with project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set your HuggingFace token \n",
    "# Get your token at: https://huggingface.co/settings/tokens\n",
    "import os\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = input(\"Enter your HF token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: deepseek-ocr\n",
      "GCS Output URI: gs://huggingface-ml-ocr/deepseek-ocr/pipeline\n"
     ]
    }
   ],
   "source": [
    "# GCP settings (update these for your project)\n",
    "PROJECT_ID = project  # From authentication cell above\n",
    "REGION = \"us-east4\"  # Cloud Run GPU available regions: us-central1, us-east4, europe-west4\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-ocr\"  # GCS bucket name\n",
    "\n",
    "# Container image settings\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/deepseek-ocr/deepseek-ocr:latest\"\n",
    "\n",
    "# Project settings\n",
    "PROJECT_NAME = \"deepseek-ocr\"\n",
    "\n",
    "# Model and dataset settings\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-OCR\"\n",
    "DATASET_NAME = \"HuggingFaceM4/FineVision\"\n",
    "DATASET_CONFIG = \"olmOCR-mix-0225-documents\"\n",
    "MAX_SAMPLES = 128\n",
    "\n",
    "# GPU configuration\n",
    "GPU_TYPE = \"nvidia-l4\"  # Cloud Run supports L4 GPUs\n",
    "GPU_COUNT = 1\n",
    "MEMORY = \"32Gi\"\n",
    "CPU = \"8\"\n",
    "\n",
    "# GCS output path (single location for all stages - dataset gets updated in place)\n",
    "GCS_OUTPUT_URI = f\"gs://{BUCKET_NAME}/{PROJECT_NAME}/pipeline\"\n",
    "\n",
    "# Base environment variables (passed to all stages)\n",
    "BASE_ENV = {\n",
    "    \"MODEL_ID\": MODEL_NAME,\n",
    "    \"DATASET_NAME\": DATASET_NAME,\n",
    "    \"DATASET_CONFIG\": DATASET_CONFIG,\n",
    "    \"MAX_SAMPLES\": str(MAX_SAMPLES),\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "    # Performance tuning\n",
    "    \"EXTRACT_BATCH_SIZE\": \"32\",\n",
    "    \"GPU_MEMORY_UTILIZATION\": \"0.90\",\n",
    "    \"VLLM_STARTUP_TIMEOUT\": \"900\",\n",
    "\n",
    "}\n",
    "\n",
    "# Add HF token to job environment\n",
    "if os.environ.get(\"HF_TOKEN\"):\n",
    "    BASE_ENV[\"HF_TOKEN\"] = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"Project: {PROJECT_NAME}\")\n",
    "print(f\"GCS Output URI: {GCS_OUTPUT_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists: gs://huggingface-ml-ocr\n"
     ]
    }
   ],
   "source": [
    "# Create GCS bucket if it doesn't exist\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"Bucket already exists: gs://{BUCKET_NAME}\")\n",
    "except Exception:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"Created bucket: gs://{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Container Image\n",
    "\n",
    "**Note:** Container building requires either:\n",
    "1. Run from a machine with Docker installed and push to Artifact Registry\n",
    "2. Use Cloud Build (requires `gcloud` CLI or Cloud Build API)\n",
    "3. Use a pre-built image\n",
    "\n",
    "For simplicity, we'll create the Dockerfile here and you can build it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile.cloudrun\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile.cloudrun\n",
    "FROM vllm/vllm-openai:latest\n",
    "\n",
    "# Install uv for fast dependency management\n",
    "RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "ENV PATH=\"/root/.local/bin:$PATH\"\n",
    "\n",
    "# Copy pipeline code (from parent directory context)\n",
    "WORKDIR /app\n",
    "COPY llm_ocr/ /app/llm_ocr/\n",
    "COPY google-cloud-run/gcr_job_runner.py /app/\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "ENV PYTHONPATH=/app:$PYTHONPATH\n",
    "\n",
    "# Entry point - uv run reads deps from inline script metadata (PEP 723)\n",
    "ENTRYPOINT [\"uv\", \"run\", \"/app/gcr_job_runner.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî® Build and Push Instructions\n",
    "\n",
    "Run these commands from the `batch-ocr-inference/` directory:\n",
    "\n",
    "```bash\n",
    "# 1. Authenticate Docker with Artifact Registry\n",
    "gcloud auth configure-docker us-east4-docker.pkg.dev\n",
    "\n",
    "# 2. Create Artifact Registry repository (if needed)\n",
    "gcloud artifacts repositories create deepseek-ocr \\\n",
    "    --repository-format=docker \\\n",
    "    --location=us-east4 \\\n",
    "    --project=$PROJECT_ID\n",
    "\n",
    "# 3. Build the Docker image\n",
    "docker build -f google-cloud-run/Dockerfile.cloudrun \\\n",
    "    -t us-east4-docker.pkg.dev/$PROJECT_ID/deepseek-ocr/deepseek-ocr:latest .\n",
    "\n",
    "# 4. Push to Artifact Registry\n",
    "docker push us-east4-docker.pkg.dev/$PROJECT_ID/deepseek-ocr/deepseek-ocr:latest\n",
    "```\n",
    "\n",
    "> üí° Replace `$PROJECT_ID` with your GCP project ID (e.g., `huggingface-ml`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions ([Python SDK](https://cloud.google.com/python/docs/reference))\n",
    "\n",
    "These functions use the [Cloud Run Python client library](https://cloud.google.com/python/docs/reference/run/latest) to create and manage jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_stage(stage: str, env: dict = None, use_gpu: bool = True):\n",
    "    \"\"\"Launch a pipeline stage as a Cloud Run job.\n",
    "    \n",
    "    Args:\n",
    "        stage: Pipeline stage (extract, describe, assemble)\n",
    "        env: Stage-specific environment variables (optional)\n",
    "        use_gpu: Whether to use GPU (default True). Set False for CPU-only jobs.\n",
    "        \n",
    "    Returns:\n",
    "        job_name: Name of the created/running job\n",
    "    \"\"\"\n",
    "    from google.cloud import run_v2\n",
    "    from google.protobuf import duration_pb2\n",
    "    \n",
    "    job_name = f\"{PROJECT_NAME}-{stage}\"\n",
    "    \n",
    "    # Create client with regional endpoint\n",
    "    client_options = {\"api_endpoint\": f\"{REGION}-run.googleapis.com\"}\n",
    "    client = run_v2.JobsClient(client_options=client_options)\n",
    "    \n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "    job_path = f\"{parent}/jobs/{job_name}\"\n",
    "    \n",
    "    # Merge base env with stage-specific env\n",
    "    full_env = {**BASE_ENV, \"PIPELINE_STAGE\": stage}\n",
    "    if env:\n",
    "        full_env.update(env)\n",
    "    \n",
    "    # Build environment variables list\n",
    "    env_vars = [run_v2.EnvVar(name=k, value=str(v)) for k, v in full_env.items()]\n",
    "    \n",
    "    # Build resource limits (GPU or CPU-only)\n",
    "    if use_gpu:\n",
    "        resource_limits = {\n",
    "            \"cpu\": CPU,\n",
    "            \"memory\": MEMORY,\n",
    "            \"nvidia.com/gpu\": str(GPU_COUNT),\n",
    "        }\n",
    "    else:\n",
    "        # CPU-only: use less resources\n",
    "        resource_limits = {\n",
    "            \"cpu\": \"4\",\n",
    "            \"memory\": \"8Gi\",\n",
    "        }\n",
    "    \n",
    "    # Build task template\n",
    "    task_template_kwargs = {\n",
    "        \"containers\": [\n",
    "            run_v2.Container(\n",
    "                image=IMAGE_URI,\n",
    "                env=env_vars,\n",
    "                resources=run_v2.ResourceRequirements(limits=resource_limits),\n",
    "            )\n",
    "        ],\n",
    "        \"timeout\": duration_pb2.Duration(seconds=3600),\n",
    "        \"max_retries\": 0,\n",
    "    }\n",
    "    \n",
    "    # Add GPU-specific settings only if using GPU\n",
    "    if use_gpu:\n",
    "        task_template_kwargs[\"gpu_zonal_redundancy_disabled\"] = True\n",
    "        task_template_kwargs[\"node_selector\"] = run_v2.NodeSelector(accelerator=GPU_TYPE)\n",
    "    \n",
    "    # Create job configuration\n",
    "    job = run_v2.Job(\n",
    "        template=run_v2.ExecutionTemplate(\n",
    "            template=run_v2.TaskTemplate(**task_template_kwargs),\n",
    "        ),\n",
    "        labels={\"stage\": stage},\n",
    "    )\n",
    "    \n",
    "    # Delete existing job if it exists\n",
    "    try:\n",
    "        client.get_job(name=job_path)\n",
    "        print(f\"Deleting existing job: {job_name}\")\n",
    "        delete_op = client.delete_job(name=job_path)\n",
    "        delete_op.result()\n",
    "        import time\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        if \"not found\" not in str(e).lower() and \"404\" not in str(e):\n",
    "            print(f\"Warning: {e}\")\n",
    "    \n",
    "    # Create job\n",
    "    gpu_str = \"GPU\" if use_gpu else \"CPU-only\"\n",
    "    print(f\"Creating job: {job_name} ({gpu_str})\")\n",
    "    request = run_v2.CreateJobRequest(parent=parent, job=job, job_id=job_name)\n",
    "    operation = client.create_job(request=request)\n",
    "    operation.result()\n",
    "    \n",
    "    # Run job\n",
    "    print(f\"Launching {stage} stage...\")\n",
    "    run_operation = client.run_job(name=job_path)\n",
    "    \n",
    "    print(f\"Started job: {job_name}\")\n",
    "    print(f\"Console: https://console.cloud.google.com/run/jobs/details/{REGION}/{job_name}/executions?project={PROJECT_ID}\")\n",
    "    \n",
    "    return job_name\n",
    "\n",
    "\n",
    "def wait_for_job(job_name: str, poll_interval: int = 30, timeout: int = 3600):\n",
    "    \"\"\"Wait for a Cloud Run job to complete.\"\"\"\n",
    "    from google.cloud import run_v2\n",
    "    import time\n",
    "    \n",
    "    client_options = {\"api_endpoint\": f\"{REGION}-run.googleapis.com\"}\n",
    "    exec_client = run_v2.ExecutionsClient(client_options=client_options)\n",
    "    \n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{job_name}\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Waiting for job {job_name}...\")\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        executions = list(exec_client.list_executions(parent=parent))\n",
    "        if not executions:\n",
    "            time.sleep(poll_interval)\n",
    "            continue\n",
    "        \n",
    "        latest = executions[0]\n",
    "        \n",
    "        if latest.succeeded_count > 0:\n",
    "            print(f\"  {job_name}: Completed ‚úì\")\n",
    "            return {\"status\": \"Completed\", \"execution\": latest}\n",
    "        elif latest.failed_count > 0:\n",
    "            print(f\"  {job_name}: Failed ‚úó\")\n",
    "            for cond in latest.conditions:\n",
    "                if cond.type_ == \"Completed\" and cond.state.name == \"CONDITION_FAILED\":\n",
    "                    print(f\"  Reason: {cond.message}\")\n",
    "            return {\"status\": \"Failed\", \"execution\": latest}\n",
    "        else:\n",
    "            print(f\"  {job_name}: Running... (running={latest.running_count})\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "    \n",
    "    raise TimeoutError(f\"Job {job_name} did not complete within {timeout}s\")\n",
    "\n",
    "\n",
    "def check_job_status(job_name: str):\n",
    "    \"\"\"Check the status of recent executions for a job.\"\"\"\n",
    "    from google.cloud import run_v2\n",
    "    \n",
    "    client_options = {\"api_endpoint\": f\"{REGION}-run.googleapis.com\"}\n",
    "    exec_client = run_v2.ExecutionsClient(client_options=client_options)\n",
    "    \n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{job_name}\"\n",
    "    \n",
    "    print(f\"Recent executions for {job_name}:\")\n",
    "    for execution in exec_client.list_executions(parent=parent):\n",
    "        status = \"UNKNOWN\"\n",
    "        if execution.succeeded_count > 0:\n",
    "            status = \"SUCCEEDED ‚úì\"\n",
    "        elif execution.failed_count > 0:\n",
    "            status = \"FAILED ‚úó\"\n",
    "        elif execution.running_count > 0:\n",
    "            status = \"RUNNING...\"\n",
    "        \n",
    "        print(f\"  {execution.name.split('/')[-1]}: {status}\")\n",
    "\n",
    "\n",
    "# Import IO and rendering utilities from llm_ocr\n",
    "import sys; sys.path.insert(0, \"..\")\n",
    "from llm_ocr.gcr_io import load_dataset_from_gcs\n",
    "from llm_ocr.document import render_sample_markdown, display_markdown, display_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Extract\n",
    "\n",
    "Run DeepSeek OCR on each document image to produce Markdown and extract figure crops. This stage runs on GPU (L4).\n",
    "\n",
    "![Extract Stage](../assets/extract-gcr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing job: deepseek-ocr-extract\n",
      "Creating job: deepseek-ocr-extract (GPU)\n",
      "Launching extract stage...\n",
      "Started job: deepseek-ocr-extract\n",
      "Console: https://console.cloud.google.com/run/jobs/details/us-east4/deepseek-ocr-extract/executions?project=huggingface-ml\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Extract\n",
    "# Output dataset will be saved to GCS\n",
    "stage1_env = {\n",
    "    \"GCS_OUTPUT_URI\": GCS_OUTPUT_URI,\n",
    "}\n",
    "\n",
    "stage1_job = launch_stage(\"extract\", stage1_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job deepseek-ocr-extract...\n",
      "  deepseek-ocr-extract: Running... (running=1)\n",
      "  deepseek-ocr-extract: Running... (running=1)\n",
      "  deepseek-ocr-extract: Running... (running=1)\n"
     ]
    }
   ],
   "source": [
    "# Wait for extract to complete\n",
    "stage1_result = wait_for_job(stage1_job)\n",
    "print(f\"Extract stage completed: {stage1_result['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples after Extract\n",
    "ds_extract = load_dataset_from_gcs(f\"{GCS_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_extract, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Describe\n",
    "\n",
    "Generate captions for each extracted figure using vision-language inference. This stage also runs on GPU.\n",
    "\n",
    "![Describe Stage](../assets/describe-gcr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Describe\n",
    "# Updates dataset in place (same location as extract)\n",
    "stage2_env = {\n",
    "    \"GCS_OUTPUT_URI\": GCS_OUTPUT_URI,\n",
    "    \"GCS_INPUT_URI\": f\"{GCS_OUTPUT_URI}/dataset\",\n",
    "}\n",
    "\n",
    "stage2_job = launch_stage(\"describe\", stage2_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 2 to complete\n",
    "# stage2_result = wait_for_job(stage2_job)\n",
    "# print(f\"Describe stage completed: {stage2_result['status']}\")\n",
    "\n",
    "check_job_status(stage2_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for describe to complete\n",
    "describe_result = wait_for_job(\"deepseek-ocr-describe\")\n",
    "print(f\"Describe stage completed: {describe_result['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples after Describe\n",
    "ds_describe = load_dataset_from_gcs(f\"{GCS_OUTPUT_URI}/dataset\")\n",
    "#display_samples(ds_describe, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Assemble\n",
    "\n",
    "Combine the original Markdown with figure captions to produce the final enriched document. This stage is CPU-only.\n",
    "\n",
    "![Assemble Stage](../assets/assemble-gcr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Assemble\n",
    "# Updates dataset in place + saves final markdown files\n",
    "stage3_env = {\n",
    "    \"GCS_OUTPUT_URI\": GCS_OUTPUT_URI,\n",
    "    \"GCS_INPUT_URI\": f\"{GCS_OUTPUT_URI}/dataset\",\n",
    "}\n",
    "\n",
    "stage3_job = launch_stage(\"assemble\", stage3_env, use_gpu=False)  # CPU-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 3 to complete (optional)\n",
    "# stage3_result = wait_for_job(stage3_job)\n",
    "# print(f\"Assemble stage completed: {stage3_result['status']}\")\n",
    "\n",
    "check_job_status(stage3_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display final samples after Assemble\n",
    "ds_final = load_dataset_from_gcs(f\"{GCS_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_final, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rendered markdown with images for sample 1\n",
    "# This properly renders figure: URIs using images from extracted_figures column\n",
    "display_markdown(ds_final[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import tempfile\n",
    "\n",
    "# Download and load final dataset\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Download dataset files\n",
    "dataset_prefix = \"pipeline/assemble/dataset\"\n",
    "local_dir = tempfile.mkdtemp()\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=dataset_prefix)\n",
    "for blob in blobs:\n",
    "    rel_path = blob.name[len(dataset_prefix):].lstrip(\"/\")\n",
    "    if rel_path:\n",
    "        local_path = Path(local_dir) / rel_path\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        blob.download_to_filename(str(local_path))\n",
    "\n",
    "dataset = load_from_disk(local_dir)\n",
    "print(f\"Loaded dataset: {dataset}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample\n",
    "sample = dataset[0]\n",
    "print(\"Sample keys:\", list(sample.keys()))\n",
    "print(\"\\nFinal markdown preview:\")\n",
    "print(sample.get(\"document_final_markdown_text\", \"\")[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Cost Analysis\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| üñ•Ô∏è **Hardware** | Nvidia L4 (24GB), 8 vCPU, 32 GiB RAM |\n",
    "| ‚ö° **Throughput** | ~26 docs/min |\n",
    "| üîÑ **Concurrency** | 32 parallel requests |\n",
    "\n",
    "**Pricing** ([Cloud Run GPU Pricing](https://cloud.google.com/run/pricing)):\n",
    "\n",
    "| Resource | Rate | Per 40 min |\n",
    "|----------|------|------------|\n",
    "| L4 GPU (no zonal redundancy) | $0.0001867/sec | $0.45 |\n",
    "| CPU (8 vCPU) | $0.000018/vCPU-sec | $0.35 |\n",
    "| Memory (32 GiB) | $0.000002/GiB-sec | $0.15 |\n",
    "| **Total** | | **~$1** |\n",
    "\n",
    "| Scale | ‚è±Ô∏è Time | üí≤ Cost |\n",
    "|-------|------|------|\n",
    "| 1,024 docs | ~40 min | ~$1 |\n",
    "| 10,000 docs | ~6.5 hours | ~$10 |\n",
    "| 100,000 docs | ~65 hours | ~$100 |\n",
    "\n",
    "> üí° Costs include GPU + CPU + Memory. Using [Committed Use Discounts](https://cloud.google.com/run/pricing) can reduce CPU/Memory costs by up to 46%.\n",
    "\n",
    "> üìÑ **Note**: 1 doc = 1 PDF page in these benchmarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete jobs (optional)\n",
    "def delete_job(job_name: str):\n",
    "    from google.cloud.run_v2 import JobsClient\n",
    "    client = JobsClient()\n",
    "    job_path = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{job_name}\"\n",
    "    try:\n",
    "        client.delete_job(name=job_path)\n",
    "        print(f\"Deleted job: {job_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete {job_name}: {e}\")\n",
    "\n",
    "# Uncomment to delete:\n",
    "# delete_job(\"deepseek-ocr-extract\")\n",
    "# delete_job(\"deepseek-ocr-describe\")\n",
    "# delete_job(\"deepseek-ocr-assemble\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
