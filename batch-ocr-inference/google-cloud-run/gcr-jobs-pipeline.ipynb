{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek OCR Pipeline - Google Cloud Run GPU\n",
    "\n",
    "This notebook runs the 3-stage OCR pipeline on Google Cloud Run with GPU support.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Google Cloud credentials configured (service account key or application default credentials)\n",
    "- Cloud Run GPU enabled in your project\n",
    "- Required Python packages: `google-cloud-run`, `google-cloud-storage`, `google-cloud-artifact-registry`\n",
    "\n",
    "**References:**\n",
    "- [Cloud Run GPU Documentation](https://cloud.google.com/run/docs/configuring/services/gpu)\n",
    "- [Supercharging Cloud Run with GPU Power](https://medium.com/google-cloud/supercharging-cloud-run-with-gpu-power-a-new-era-for-ai-workloads-3c54fcf60cae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-run google-cloud-storage google-cloud-build google-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from google.cloud import run_v2\n",
    "from google.cloud import storage\n",
    "from google.protobuf import duration_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set credentials from a service account key file\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/service-account-key.json\"\n",
    "\n",
    "# Option 2: If running locally, run this in terminal first:\n",
    "# gcloud auth application-default login\n",
    "\n",
    "# Verify credentials\n",
    "from google.auth import default\n",
    "credentials, project = default()\n",
    "print(f\"Authenticated with project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP settings (update these for your project)\n",
    "PROJECT_ID = project  # From authentication cell above\n",
    "REGION = \"us-east4\"  # Cloud Run GPU available regions: us-central1, us-east4, europe-west4\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-ocr\"  # GCS bucket name\n",
    "\n",
    "# Container image settings\n",
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/deepseek-ocr/deepseek-ocr:latest\"\n",
    "\n",
    "# Project settings\n",
    "PROJECT_NAME = \"deepseek-ocr\"\n",
    "\n",
    "# Model and dataset settings\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-OCR\"\n",
    "DATASET_NAME = \"HuggingFaceM4/FineVision\"\n",
    "DATASET_CONFIG = \"olmOCR-mix-0225-documents\"\n",
    "MAX_SAMPLES = 20\n",
    "\n",
    "# GPU configuration\n",
    "GPU_TYPE = \"nvidia-l4\"  # Cloud Run supports L4 GPUs\n",
    "GPU_COUNT = 1\n",
    "MEMORY = \"32Gi\"\n",
    "CPU = \"8\"\n",
    "\n",
    "# GCS output path (single location for all stages - dataset gets updated in place)\n",
    "GCS_OUTPUT_URI = f\"gs://{BUCKET_NAME}/{PROJECT_NAME}/pipeline\"\n",
    "\n",
    "# Base environment variables (passed to all stages)\n",
    "BASE_ENV = {\n",
    "    \"MODEL_ID\": MODEL_NAME,\n",
    "    \"DATASET_NAME\": DATASET_NAME,\n",
    "    \"DATASET_CONFIG\": DATASET_CONFIG,\n",
    "    \"MAX_SAMPLES\": str(MAX_SAMPLES),\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "    # Performance tuning\n",
    "    \"EXTRACT_BATCH_SIZE\": \"16\",\n",
    "    \"EXTRACT_MAX_CONCURRENCY\": \"8\",\n",
    "    \"GPU_MEMORY_UTILIZATION\": \"0.90\",\n",
    "    \"VLLM_STARTUP_TIMEOUT\": \"900\",\n",
    "}\n",
    "\n",
    "# Add HF token if available\n",
    "if os.environ.get(\"HF_TOKEN\"):\n",
    "    BASE_ENV[\"HF_TOKEN\"] = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "print(f\"Project: {PROJECT_NAME}\")\n",
    "print(f\"GCS Output URI: {GCS_OUTPUT_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GCS bucket if it doesn't exist\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"Bucket already exists: gs://{BUCKET_NAME}\")\n",
    "except Exception:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"Created bucket: gs://{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Container Image\n",
    "\n",
    "**Note:** Container building requires either:\n",
    "1. Run from a machine with Docker installed and push to Artifact Registry\n",
    "2. Use Cloud Build (requires `gcloud` CLI or Cloud Build API)\n",
    "3. Use a pre-built image\n",
    "\n",
    "For simplicity, we'll create the Dockerfile here and you can build it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for Cloud Run GPU\n",
    "# Note: Build context is parent directory (..) since llm_ocr is there\n",
    "dockerfile_content = '''FROM vllm/vllm-openai:latest\n",
    "\n",
    "# Install uv for fast dependency management\n",
    "RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "ENV PATH=\"/root/.local/bin:$PATH\"\n",
    "\n",
    "# Copy pipeline code (from parent directory context)\n",
    "WORKDIR /app\n",
    "COPY llm_ocr/ /app/llm_ocr/\n",
    "COPY google-cloud-run/gcr_job_runner.py /app/\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "ENV PYTHONPATH=/app:$PYTHONPATH\n",
    "\n",
    "# Entry point - uv run reads deps from inline script metadata (PEP 723)\n",
    "ENTRYPOINT [\"uv\", \"run\", \"/app/gcr_job_runner.py\"]\n",
    "'''\n",
    "\n",
    "dockerfile_path = Path(\"Dockerfile.cloudrun\")\n",
    "dockerfile_path.write_text(dockerfile_content)\n",
    "print(f\"Created {dockerfile_path}\")\n",
    "print(\"\\nTo build and push the image, run these commands from batch-ocr-inference/:\")\n",
    "print(f\"\")\n",
    "print(f\"# Authenticate Docker with Artifact Registry\")\n",
    "print(f\"gcloud auth configure-docker {REGION}-docker.pkg.dev\")\n",
    "print(f\"\")\n",
    "print(f\"# Create Artifact Registry repository (if needed)\")\n",
    "print(f\"gcloud artifacts repositories create ocr-pipeline --repository-format=docker --location={REGION} --project={PROJECT_ID}\")\n",
    "print(f\"\")\n",
    "print(f\"# Build and push image (from batch-ocr-inference/ directory)\")\n",
    "print(f\"docker build -f google-cloud-run/Dockerfile.cloudrun -t {IMAGE_URI} .\")\n",
    "print(f\"docker push {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions (Python SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_stage(stage: str, env: dict = None):\n",
    "    \"\"\"Launch a pipeline stage as a Cloud Run job.\n",
    "    \n",
    "    Args:\n",
    "        stage: Pipeline stage (extract, describe, assemble)\n",
    "        env: Stage-specific environment variables (optional)\n",
    "        \n",
    "    Returns:\n",
    "        job_name: Name of the created/running job\n",
    "    \"\"\"\n",
    "    from google.cloud import run_v2\n",
    "    from google.protobuf import duration_pb2\n",
    "    \n",
    "    job_name = f\"{PROJECT_NAME}-{stage}\"\n",
    "    \n",
    "    # Create client with regional endpoint\n",
    "    client_options = {\"api_endpoint\": f\"{REGION}-run.googleapis.com\"}\n",
    "    client = run_v2.JobsClient(client_options=client_options)\n",
    "    \n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "    job_path = f\"{parent}/jobs/{job_name}\"\n",
    "    \n",
    "    # Merge base env with stage-specific env\n",
    "    full_env = {**BASE_ENV, \"PIPELINE_STAGE\": stage}\n",
    "    if env:\n",
    "        full_env.update(env)\n",
    "    \n",
    "    # Build environment variables list\n",
    "    env_vars = [run_v2.EnvVar(name=k, value=str(v)) for k, v in full_env.items()]\n",
    "    \n",
    "    # Create job configuration\n",
    "    job = run_v2.Job(\n",
    "        template=run_v2.ExecutionTemplate(\n",
    "            template=run_v2.TaskTemplate(\n",
    "                # Disable GPU zonal redundancy (required for GPU jobs)\n",
    "                gpu_zonal_redundancy_disabled=True,\n",
    "                containers=[\n",
    "                    run_v2.Container(\n",
    "                        image=IMAGE_URI,\n",
    "                        env=env_vars,\n",
    "                        resources=run_v2.ResourceRequirements(\n",
    "                            limits={\n",
    "                                \"cpu\": CPU,\n",
    "                                \"memory\": MEMORY,\n",
    "                                \"nvidia.com/gpu\": str(GPU_COUNT),\n",
    "                            }\n",
    "                        ),\n",
    "                    )\n",
    "                ],\n",
    "                node_selector=run_v2.NodeSelector(\n",
    "                    accelerator=GPU_TYPE,\n",
    "                ),\n",
    "                timeout=duration_pb2.Duration(seconds=3600),\n",
    "                max_retries=0,\n",
    "            ),\n",
    "        ),\n",
    "        labels={\"stage\": stage},\n",
    "    )\n",
    "    \n",
    "    # Delete existing job if it exists\n",
    "    try:\n",
    "        client.get_job(name=job_path)\n",
    "        print(f\"Deleting existing job: {job_name}\")\n",
    "        delete_op = client.delete_job(name=job_path)\n",
    "        delete_op.result()\n",
    "        import time\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        if \"not found\" not in str(e).lower() and \"404\" not in str(e):\n",
    "            print(f\"Warning: {e}\")\n",
    "    \n",
    "    # Create job\n",
    "    print(f\"Creating job: {job_name}\")\n",
    "    request = run_v2.CreateJobRequest(parent=parent, job=job, job_id=job_name)\n",
    "    operation = client.create_job(request=request)\n",
    "    operation.result()\n",
    "    \n",
    "    # Run job\n",
    "    print(f\"Launching {stage} stage...\")\n",
    "    run_operation = client.run_job(name=job_path)\n",
    "    \n",
    "    print(f\"Started job: {job_name}\")\n",
    "    print(f\"Console: https://console.cloud.google.com/run/jobs/details/{REGION}/{job_name}/executions?project={PROJECT_ID}\")\n",
    "    \n",
    "    return job_name\n",
    "\n",
    "\n",
    "def wait_for_job(job_name: str, poll_interval: int = 30, timeout: int = 3600):\n",
    "    \"\"\"Wait for a Cloud Run job to complete.\"\"\"\n",
    "    from google.cloud import run_v2\n",
    "    import time\n",
    "    \n",
    "    client_options = {\"api_endpoint\": f\"{REGION}-run.googleapis.com\"}\n",
    "    exec_client = run_v2.ExecutionsClient(client_options=client_options)\n",
    "    \n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{job_name}\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Waiting for job {job_name}...\")\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        executions = list(exec_client.list_executions(parent=parent))\n",
    "        if not executions:\n",
    "            time.sleep(poll_interval)\n",
    "            continue\n",
    "        \n",
    "        latest = executions[0]\n",
    "        \n",
    "        if latest.succeeded_count > 0:\n",
    "            print(f\"  {job_name}: Completed ✓\")\n",
    "            return {\"status\": \"Completed\", \"execution\": latest}\n",
    "        elif latest.failed_count > 0:\n",
    "            print(f\"  {job_name}: Failed ✗\")\n",
    "            for cond in latest.conditions:\n",
    "                if cond.type_ == \"Completed\" and cond.state.name == \"CONDITION_FAILED\":\n",
    "                    print(f\"  Reason: {cond.message}\")\n",
    "            return {\"status\": \"Failed\", \"execution\": latest}\n",
    "        else:\n",
    "            print(f\"  {job_name}: Running... (running={latest.running_count}, pending={latest.pending_count})\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "    \n",
    "    raise TimeoutError(f\"Job {job_name} did not complete within {timeout}s\")\n",
    "\n",
    "\n",
    "def check_job_status(job_name: str):\n",
    "    \"\"\"Check the status of recent executions for a job.\"\"\"\n",
    "    from google.cloud import run_v2\n",
    "    \n",
    "    client_options = {\"api_endpoint\": f\"{REGION}-run.googleapis.com\"}\n",
    "    exec_client = run_v2.ExecutionsClient(client_options=client_options)\n",
    "    \n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{job_name}\"\n",
    "    \n",
    "    print(f\"Recent executions for {job_name}:\")\n",
    "    for execution in exec_client.list_executions(parent=parent):\n",
    "        status = \"UNKNOWN\"\n",
    "        if execution.succeeded_count > 0:\n",
    "            status = \"SUCCEEDED ✓\"\n",
    "        elif execution.failed_count > 0:\n",
    "            status = \"FAILED ✗\"\n",
    "        elif execution.running_count > 0:\n",
    "            status = \"RUNNING...\"\n",
    "        elif execution.pending_count > 0:\n",
    "            status = \"PENDING\"\n",
    "        print(f\"  {execution.name.split('/')[-1]}: {status}\")\n",
    "\n",
    "\n",
    "# Import IO and rendering utilities from llm_ocr\n",
    "from llm_ocr.gcr_io import load_dataset_from_gcs\n",
    "from llm_ocr.document import render_sample_markdown, display_markdown\n",
    "\n",
    "\n",
    "def display_samples(dataset, num_samples: int = 2):\n",
    "    \"\"\"Display a few samples from the dataset.\"\"\"\n",
    "    from IPython.display import display\n",
    "    \n",
    "    print(f\"Dataset: {len(dataset)} samples\")\n",
    "    print(f\"Columns: {list(dataset.column_names)}\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        print(f\"=== Sample {i}: {sample['sample_id']} ===\")\n",
    "        \n",
    "        if sample.get('source_image'):\n",
    "            print(\"Source image:\")\n",
    "            display(sample['source_image'])\n",
    "        \n",
    "        md = sample.get('document_markdown') or sample.get('document_markdown_text', '')\n",
    "        if md:\n",
    "            print(f\"\\nMarkdown preview ({len(md)} chars):\")\n",
    "            print(md[:500] + '...' if len(md) > 500 else md)\n",
    "        \n",
    "        final_md = sample.get('document_final_markdown') or sample.get('document_final_markdown_text', '')\n",
    "        if final_md:\n",
    "            print(f\"\\nFinal markdown preview ({len(final_md)} chars):\")\n",
    "            print(final_md[:500] + '...' if len(final_md) > 500 else final_md)\n",
    "        \n",
    "        figures = sample.get('extracted_figures', [])\n",
    "        if figures:\n",
    "            print(f\"\\nExtracted figures: {len(figures)}\")\n",
    "            for fig in figures[:2]:\n",
    "                display(fig)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Extract\n",
    "# Output dataset will be saved to GCS\n",
    "stage1_env = {\n",
    "    \"GCS_OUTPUT_URI\": GCS_OUTPUT_URI,\n",
    "}\n",
    "\n",
    "stage1_job = launch_stage(\"extract\", stage1_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for extract to complete\n",
    "stage1_result = wait_for_job(stage1_job)\n",
    "print(f\"Extract stage completed: {stage1_result['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples after Extract\n",
    "ds_extract = load_dataset_from_gcs(f\"{GCS_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_extract, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Describe\n",
    "# Updates dataset in place (same location as extract)\n",
    "stage2_env = {\n",
    "    \"GCS_OUTPUT_URI\": GCS_OUTPUT_URI,\n",
    "    \"GCS_INPUT_URI\": f\"{GCS_OUTPUT_URI}/dataset\",\n",
    "}\n",
    "\n",
    "stage2_job = launch_stage(\"describe\", stage2_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 2 to complete\n",
    "# stage2_result = wait_for_job(stage2_job)\n",
    "# print(f\"Describe stage completed: {stage2_result['status']}\")\n",
    "\n",
    "check_job_status(stage2_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for describe to complete\n",
    "describe_result = wait_for_job(\"deepseek-ocr-describe\")\n",
    "print(f\"Describe stage completed: {describe_result['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples after Describe\n",
    "ds_describe = load_dataset_from_gcs(f\"{GCS_OUTPUT_URI}/dataset\")\n",
    "#display_samples(ds_describe, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Assemble\n",
    "# Updates dataset in place + saves final markdown files\n",
    "stage3_env = {\n",
    "    \"GCS_OUTPUT_URI\": GCS_OUTPUT_URI,\n",
    "    \"GCS_INPUT_URI\": f\"{GCS_OUTPUT_URI}/dataset\",\n",
    "}\n",
    "\n",
    "stage3_job = launch_stage(\"assemble\", stage3_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 3 to complete (optional)\n",
    "# stage3_result = wait_for_job(stage3_job)\n",
    "# print(f\"Assemble stage completed: {stage3_result['status']}\")\n",
    "\n",
    "check_job_status(stage3_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display final samples after Assemble\n",
    "ds_final = load_dataset_from_gcs(f\"{GCS_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_final, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rendered markdown with images for sample 1\n",
    "# This properly renders figure: URIs using images from extracted_figures column\n",
    "display_markdown(ds_final[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import tempfile\n",
    "\n",
    "# Download and load final dataset\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Download dataset files\n",
    "dataset_prefix = \"pipeline/assemble/dataset\"\n",
    "local_dir = tempfile.mkdtemp()\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=dataset_prefix)\n",
    "for blob in blobs:\n",
    "    rel_path = blob.name[len(dataset_prefix):].lstrip(\"/\")\n",
    "    if rel_path:\n",
    "        local_path = Path(local_dir) / rel_path\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        blob.download_to_filename(str(local_path))\n",
    "\n",
    "dataset = load_from_disk(local_dir)\n",
    "print(f\"Loaded dataset: {dataset}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample\n",
    "sample = dataset[0]\n",
    "print(\"Sample keys:\", list(sample.keys()))\n",
    "print(\"\\nFinal markdown preview:\")\n",
    "print(sample.get(\"document_final_markdown_text\", \"\")[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete jobs (optional)\n",
    "def delete_job(job_name: str):\n",
    "    from google.cloud.run_v2 import JobsClient\n",
    "    client = JobsClient()\n",
    "    job_path = f\"projects/{PROJECT_ID}/locations/{REGION}/jobs/{job_name}\"\n",
    "    try:\n",
    "        client.delete_job(name=job_path)\n",
    "        print(f\"Deleted job: {job_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete {job_name}: {e}\")\n",
    "\n",
    "# Uncomment to delete:\n",
    "# delete_job(\"deepseek-ocr-extract\")\n",
    "# delete_job(\"deepseek-ocr-describe\")\n",
    "# delete_job(\"deepseek-ocr-assemble\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
