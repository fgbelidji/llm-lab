"""Unified storage abstraction for dataset I/O.

This module provides a common interface for saving/loading HuggingFace datasets,
abstracting away whether we're using HuggingFace Hub, S3, or GCS.

Usage:
    from .storage import get_storage
    
    storage = get_storage()
    storage.save_dataset(dataset, "my_dataset")
    dataset = storage.load_dataset()
"""
from __future__ import annotations

import logging
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Optional

from .config import env

if TYPE_CHECKING:
    from datasets import Dataset

LOGGER = logging.getLogger(__name__)


class DatasetStorage(ABC):
    """Abstract base class for dataset storage backends."""
    
    @abstractmethod
    def save_dataset(self, dataset: "Dataset", name: str) -> bool:
        """Save a HuggingFace dataset to storage.
        
        Args:
            dataset: HuggingFace Dataset to save
            name: Name/identifier for the dataset
            
        Returns:
            True if save succeeded
        """
        pass
    
    @abstractmethod
    def load_dataset(self, split: str = "train") -> Optional["Dataset"]:
        """Load a HuggingFace dataset from storage.
        
        Args:
            split: Dataset split to load
            
        Returns:
            Loaded Dataset or None if not available
        """
        pass
    
    @property
    @abstractmethod
    def is_configured(self) -> bool:
        """Check if this storage backend is configured."""
        pass


class HFHubStorage(DatasetStorage):
    """HuggingFace Hub storage backend."""
    
    def __init__(
        self,
        repo_id: Optional[str] = None,
        branch: Optional[str] = None,
        commit_message: Optional[str] = None,
    ):
        self.repo_id = repo_id or env("HF_REPO_ID")
        self.branch = branch or env("HF_BRANCH")
        self.commit_message = commit_message or env("HF_COMMIT_MESSAGE")
        self._token = env("HF_TOKEN")
    
    @property
    def is_configured(self) -> bool:
        return bool(self.repo_id)
    
    def save_dataset(self, dataset: "Dataset", name: str) -> bool:
        if not self.is_configured:
            LOGGER.debug("HF Hub not configured, skipping dataset save")
            return False
        
        try:
            dataset.push_to_hub(
                self.repo_id,
                token=self._token,
                revision=self.branch,
                commit_message=self.commit_message or f"Add {name}",
            )
            LOGGER.info("Pushed dataset to HF Hub: %s", self.repo_id)
            return True
        except Exception as exc:
            LOGGER.exception("HF Hub dataset push failed: %s", exc)
            return False
    
    def load_dataset(self, split: str = "train") -> Optional["Dataset"]:
        if not self.is_configured:
            LOGGER.debug("HF Hub not configured, cannot load dataset")
            return None
        
        try:
            from datasets import load_dataset
            
            LOGGER.info("Loading dataset from HF Hub: %s", self.repo_id)
            return load_dataset(self.repo_id, split=split, token=self._token)
        except Exception as exc:
            LOGGER.exception("HF Hub dataset load failed: %s", exc)
            return None


class S3Storage(DatasetStorage):
    """Amazon S3 storage backend."""
    
    def __init__(
        self,
        output_uri: Optional[str] = None,
        input_uri: Optional[str] = None,
    ):
        self.output_uri = output_uri or env("S3_OUTPUT_URI")
        self.input_uri = input_uri or env("S3_INPUT_URI")
    
    @property
    def is_configured(self) -> bool:
        return bool(self.output_uri or self.input_uri)
    
    def save_dataset(self, dataset: "Dataset", name: str) -> bool:
        if not self.output_uri:
            LOGGER.debug("S3 output URI not configured, skipping dataset save")
            return False
        
        try:
            from .sm_io import save_dataset_to_s3
            
            save_dataset_to_s3(dataset, self.output_uri, name)
            return True
        except ImportError as exc:
            LOGGER.warning("S3 save failed (missing dependency): %s", exc)
            return False
        except Exception as exc:
            LOGGER.exception("S3 dataset save failed: %s", exc)
            return False
    
    def load_dataset(self, split: str = "train") -> Optional["Dataset"]:
        if not self.input_uri:
            LOGGER.debug("S3 input URI not configured, cannot load dataset")
            return None
        
        try:
            from .sm_io import load_dataset_from_s3
            
            return load_dataset_from_s3(self.input_uri, split=split)
        except ImportError as exc:
            LOGGER.warning("S3 load failed (missing dependency): %s", exc)
            return None
        except Exception as exc:
            LOGGER.exception("S3 dataset load failed: %s", exc)
            return None


class GCSStorage(DatasetStorage):
    """Google Cloud Storage backend."""
    
    def __init__(
        self,
        output_uri: Optional[str] = None,
        input_uri: Optional[str] = None,
    ):
        self.output_uri = output_uri or env("GCS_OUTPUT_URI")
        self.input_uri = input_uri or env("GCS_INPUT_URI")
    
    @property
    def is_configured(self) -> bool:
        return bool(self.output_uri or self.input_uri)
    
    def save_dataset(self, dataset: "Dataset", name: str) -> bool:
        if not self.output_uri:
            LOGGER.debug("GCS output URI not configured, skipping dataset save")
            return False
        
        try:
            from .gcr_io import save_dataset_to_gcs
            
            save_dataset_to_gcs(dataset, self.output_uri, name)
            return True
        except ImportError as exc:
            LOGGER.warning("GCS save failed (missing dependency): %s", exc)
            return False
        except Exception as exc:
            LOGGER.exception("GCS dataset save failed: %s", exc)
            return False
    
    def load_dataset(self, split: str = "train") -> Optional["Dataset"]:
        if not self.input_uri:
            LOGGER.debug("GCS input URI not configured, cannot load dataset")
            return None
        
        try:
            from .gcr_io import load_dataset_from_gcs
            
            return load_dataset_from_gcs(self.input_uri, split=split)
        except ImportError as exc:
            LOGGER.warning("GCS load failed (missing dependency): %s", exc)
            return None
        except Exception as exc:
            LOGGER.exception("GCS dataset load failed: %s", exc)
            return None


def get_storage(
    *,
    repo_id: Optional[str] = None,
    s3_output_uri: Optional[str] = None,
    s3_input_uri: Optional[str] = None,
    gcs_output_uri: Optional[str] = None,
    gcs_input_uri: Optional[str] = None,
) -> DatasetStorage:
    """Get the appropriate storage backend based on configuration.
    
    Priority: GCS > S3 > HF Hub.
    
    Args:
        repo_id: Override HF repo ID
        s3_output_uri: Override S3 output URI
        s3_input_uri: Override S3 input URI
        gcs_output_uri: Override GCS output URI
        gcs_input_uri: Override GCS input URI
        
    Returns:
        Configured DatasetStorage instance
    """
    gcs = GCSStorage(output_uri=gcs_output_uri, input_uri=gcs_input_uri)
    s3 = S3Storage(output_uri=s3_output_uri, input_uri=s3_input_uri)
    hf = HFHubStorage(repo_id=repo_id)
    
    # Return first configured backend
    if gcs.is_configured:
        return gcs
    if s3.is_configured:
        return s3
    return hf


def get_source_storage(
    *,
    source_repo_id: Optional[str] = None,
) -> DatasetStorage:
    """Get storage backend for loading source data.
    
    Checks GCS_INPUT_URI first, then S3_INPUT_URI, then falls back to HF Hub.
    
    Args:
        source_repo_id: HF repo ID to load from (falls back to SOURCE_REPO_ID env var)
        
    Returns:
        Configured DatasetStorage instance for loading
    """
    gcs_input = env("GCS_INPUT_URI")
    if gcs_input:
        return GCSStorage(input_uri=gcs_input)
    
    s3_input = env("S3_INPUT_URI")
    if s3_input:
        return S3Storage(input_uri=s3_input)
    
    repo_id = source_repo_id or env("SOURCE_REPO_ID") or env("HF_REPO_ID")
    return HFHubStorage(repo_id=repo_id)


__all__ = [
    "DatasetStorage",
    "HFHubStorage",
    "S3Storage",
    "GCSStorage",
    "get_storage",
    "get_source_storage",
]
