{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek OCR Pipeline on SageMaker Training Jobs\n",
    "\n",
    "This notebook runs a three-stage OCR pipeline using SageMaker Training Jobs:\n",
    "\n",
    "1. **Extract** – Run DeepSeek OCR over a dataset, save Markdown and crop detected figures\n",
    "2. **Describe** – Generate captions for extracted figures  \n",
    "3. **Assemble** – Enrich Markdown with figure captions\n",
    "\n",
    "This is the SageMaker equivalent of the HuggingFace Jobs pipeline. It uses SageMaker ModelTrainer V3\n",
    "with a vLLM container to run GPU-accelerated inference.\n",
    "\n",
    "**Key difference from HF Jobs:** This notebook saves datasets to S3 instead of HuggingFace Hub.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured\n",
    "- SageMaker execution role with S3 access\n",
    "- HuggingFace token for accessing source models and datasets\n",
    "- SageMaker SDK V3 installed (`pip install sagemaker --upgrade`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"datasets>=4.0.0\" \"s3fs\" \"fsspec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import SourceCode, Compute, StoppingCondition, OutputDataConfig\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = Session()\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='sagemaker-dlcs')['Role']['Arn']\n",
    "region = sagemaker_session.boto_region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account: {account_id}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Configuration\n",
    "PROJECT_NAME = \"deepseek-ocr-sagemaker\"\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "S3_PREFIX = f\"{PROJECT_NAME}\"\n",
    "\n",
    "# S3 output path (single location for all stages - dataset gets updated in place)\n",
    "S3_OUTPUT_URI = f\"s3://{BUCKET_NAME}/{S3_PREFIX}\"\n",
    "\n",
    "# vLLM Container - use SageMaker vLLM DLC\n",
    "TRAINING_IMAGE = f\"763104351884.dkr.ecr.{region}.amazonaws.com/vllm:0.12.0-gpu-py312-cu129-ubuntu22.04-sagemaker-v1.0\"\n",
    "\n",
    "# Instance configuration\n",
    "INSTANCE_TYPE = \"ml.g5.2xlarge\"  # Single A10G GPU\n",
    "# INSTANCE_TYPE = \"ml.p4d.24xlarge\"  # 8x A100 GPUs for larger scale\n",
    "VOLUME_SIZE_GB = 100\n",
    "MAX_RUNTIME_SECONDS = 3 * 60 * 60  # 3 hours\n",
    "\n",
    "# Source dataset (from HuggingFace)\n",
    "SOURCE_DATASET = \"HuggingFaceM4/FineVision\"\n",
    "SOURCE_CONFIG = \"olmOCR-mix-0225-documents\"\n",
    "MAX_SAMPLES = 20  # Start small for testing\n",
    "\n",
    "# HuggingFace token for accessing source datasets\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "\n",
    "print(f\"S3 Bucket: s3://{BUCKET_NAME}/{S3_PREFIX}\")\n",
    "print(f\"S3 Output URI: {S3_OUTPUT_URI}\")\n",
    "print(f\"Instance: {INSTANCE_TYPE}\")\n",
    "print(f\"Source: {SOURCE_DATASET}/{SOURCE_CONFIG} ({MAX_SAMPLES} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bundle Pipeline Code\n",
    "\n",
    "SageMaker automatically uploads this bundle to S3 and makes it available at `/opt/ml/input/data/code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to pipeline code\n",
    "CODE_PATHS = [\n",
    "    Path(\"entry.sh\"),\n",
    "    Path(\"sm_job_runner.py\"),\n",
    "    Path(\"../llm_ocr\"),\n",
    "]\n",
    "\n",
    "# Create a source directory bundle\n",
    "source_dir = Path(tempfile.mkdtemp(prefix=\"sm-ocr-code-\"))\n",
    "\n",
    "for path in CODE_PATHS:\n",
    "    src = Path.cwd() / path if not path.is_absolute() else path\n",
    "    if src.is_dir():\n",
    "        shutil.copytree(src, source_dir / path.name, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(src, source_dir / path.name)\n",
    "\n",
    "print(f\"Source directory: {source_dir}\")\n",
    "print(f\"Contents: {list(source_dir.iterdir())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies are declared in sm_job_runner.py inline metadata (PEP 723)\n",
    "# entry.sh installs uv and runs: uv run sm_job_runner.py\n",
    "# This automatically installs all dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Base Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base environment variables for all stages\n",
    "# All configuration is passed via environment variables (same as HF Jobs)\n",
    "BASE_ENV = {\n",
    "    # vLLM configuration\n",
    "    \"MODEL_ID\": \"deepseek-ai/DeepSeek-OCR\",\n",
    "    \"SERVED_MODEL_NAME\": \"deepseek-ocr\",\n",
    "    \"HOST\": \"0.0.0.0\",\n",
    "    \"PORT\": \"8000\",\n",
    "    \"MAX_MODEL_LEN\": \"8192\",\n",
    "    \"GPU_MEMORY_UTILIZATION\": \"0.90\",\n",
    "    \"TENSOR_PARALLEL_SIZE\": \"1\",\n",
    "    \n",
    "    # HuggingFace authentication (for source datasets)\n",
    "    \"HF_TOKEN\": HF_TOKEN,\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "    \n",
    "    # Prompts\n",
    "    \"DOC_PROMPT\": \"<image>\\n<|grounding|>Convert this document to Markdown.\",\n",
    "    \"DOC_MAX_TOKENS\": \"4096\",\n",
    "    \"DOC_TEMPERATURE\": \"0.1\",\n",
    "    \"FIGURE_PROMPT\": \"<image>\\nDescribe this image in detail.\",\n",
    "    \"FIGURE_MAX_TOKENS\": \"512\",\n",
    "    \"FIGURE_TEMPERATURE\": \"0.6\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_stage(stage: str, env: dict = None):\n",
    "    \"\"\"Launch a pipeline stage as a SageMaker Training Job.\n",
    "    \n",
    "    Args:\n",
    "        stage: Pipeline stage (extract, describe, assemble)\n",
    "        env: Stage-specific environment variables (optional)\n",
    "        \n",
    "    Returns:\n",
    "        ModelTrainer object\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    \n",
    "    job_name = f\"{PROJECT_NAME}-{stage}-{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    # Merge base env with stage-specific env\n",
    "    full_env = {**BASE_ENV, \"PIPELINE_STAGE\": stage}\n",
    "    if env:\n",
    "        full_env.update(env)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "        image_uri=TRAINING_IMAGE,\n",
    "        training_mode=\"Heterogeneous\",\n",
    "        source_code=SourceCode(\n",
    "            source_dir=str(source_dir),\n",
    "            entry_script=\"entry.sh\",\n",
    "        ),\n",
    "        compute=Compute(\n",
    "            instance_type=INSTANCE_TYPE,\n",
    "            instance_count=1,\n",
    "            volume_size_in_gb=VOLUME_SIZE_GB,\n",
    "        ),\n",
    "        stopping_condition=StoppingCondition(\n",
    "            max_runtime_in_seconds=MAX_RUNTIME_SECONDS,\n",
    "        ),\n",
    "        output_data_config=OutputDataConfig(\n",
    "            s3_output_path=f\"s3://{BUCKET_NAME}/{S3_PREFIX}/output/\",\n",
    "        ),\n",
    "        base_job_name=job_name,\n",
    "        environment=full_env,\n",
    "    )\n",
    "    \n",
    "    print(f\"Launching {stage} stage: {job_name}\")\n",
    "    trainer.train(wait=False)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "def wait_for_job(trainer, poll_interval: int = 30, timeout: int = 10800):\n",
    "    \"\"\"Wait for a SageMaker Training Job to complete.\"\"\"\n",
    "    import time\n",
    "    \n",
    "    job_name = trainer._latest_job.name\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Waiting for job {job_name}...\")\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        response = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = response['TrainingJobStatus']\n",
    "        \n",
    "        if status == 'Completed':\n",
    "            print(f\"  {job_name}: Completed ✓\")\n",
    "            return response\n",
    "        elif status == 'Failed':\n",
    "            print(f\"  {job_name}: Failed ✗\")\n",
    "            print(f\"  Reason: {response.get('FailureReason', 'Unknown')}\")\n",
    "            return response\n",
    "        elif status == 'Stopped':\n",
    "            print(f\"  {job_name}: Stopped\")\n",
    "            return response\n",
    "        else:\n",
    "            print(f\"  {job_name}: {status}...\")\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "    \n",
    "    raise TimeoutError(f\"Job {job_name} did not complete within {timeout}s\")\n",
    "\n",
    "\n",
    "# Import IO and rendering utilities from llm_ocr\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory for llm_ocr imports\n",
    "from llm_ocr.sm_io import load_dataset_from_s3\n",
    "from llm_ocr.document import render_sample_markdown, display_markdown\n",
    "\n",
    "\n",
    "def display_samples(dataset, num_samples: int = 2):\n",
    "    \"\"\"Display a few samples from the dataset.\"\"\"\n",
    "    from IPython.display import display\n",
    "    \n",
    "    print(f\"Dataset: {len(dataset)} samples\")\n",
    "    print(f\"Columns: {list(dataset.column_names)}\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        print(f\"=== Sample {i}: {sample['sample_id']} ===\")\n",
    "        \n",
    "        if sample.get('source_image'):\n",
    "            print(\"Source image:\")\n",
    "            display(sample['source_image'])\n",
    "        \n",
    "        md = sample.get('document_markdown') or sample.get('document_markdown_text', '')\n",
    "        if md:\n",
    "            print(f\"\\nMarkdown preview ({len(md)} chars):\")\n",
    "            print(md[:500] + '...' if len(md) > 500 else md)\n",
    "        \n",
    "        final_md = sample.get('document_final_markdown') or sample.get('document_final_markdown_text', '')\n",
    "        if final_md:\n",
    "            print(f\"\\nFinal markdown preview ({len(final_md)} chars):\")\n",
    "            print(final_md[:500] + '...' if len(final_md) > 500 else final_md)\n",
    "        \n",
    "        figures = sample.get('extracted_figures', [])\n",
    "        if figures:\n",
    "            print(f\"\\nExtracted figures: {len(figures)}\")\n",
    "            for fig in figures[:2]:\n",
    "                display(fig)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Extract\n",
    "\n",
    "Run OCR on the source dataset to extract markdown and figures.\n",
    "Output is saved to S3 (not HF Hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Extract\n",
    "# Output dataset will be saved to S3\n",
    "stage1_env = {\n",
    "    # Source dataset (from HuggingFace)\n",
    "    \"DATASET_NAME\": SOURCE_DATASET,\n",
    "    \"DATASET_CONFIG\": SOURCE_CONFIG,\n",
    "    \"DATASET_SPLIT\": \"train\",\n",
    "    \"MAX_SAMPLES\": str(MAX_SAMPLES),\n",
    "    \n",
    "    # Local output directory\n",
    "    \"OUTPUT_DIR\": \"./outputs\",\n",
    "    \n",
    "    # Batch settings\n",
    "    \"EXTRACT_BATCH_SIZE\": \"16\",\n",
    "    \"EXTRACT_MAX_CONCURRENCY\": \"4\",\n",
    "    \n",
    "    # S3 output (single location for all stages)\n",
    "    \"S3_OUTPUT_URI\": S3_OUTPUT_URI,\n",
    "}\n",
    "\n",
    "stage1_trainer = launch_stage(\"extract\", stage1_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 1 to complete\n",
    "stage1_result = wait_for_job(stage1_trainer)\n",
    "print(f\"Extract stage completed: {stage1_result['TrainingJobStatus']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples after Extract\n",
    "ds_extract = load_dataset_from_s3(f\"{S3_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_extract, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Describe\n",
    "\n",
    "Generate captions for extracted figures.\n",
    "Input is read from S3 (output of Stage 1), output is saved to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Describe\n",
    "# Updates dataset in place (same location as extract)\n",
    "stage2_env = {\n",
    "    # Local output directory\n",
    "    \"OUTPUT_DIR\": \"./outputs\",\n",
    "    \n",
    "    # Batch settings\n",
    "    \"DESCRIBE_BATCH_SIZE\": \"8\",\n",
    "    \"DESCRIBE_MAX_CONCURRENCY\": \"4\",\n",
    "    \n",
    "    # S3 input and output (same location - updates in place)\n",
    "    \"S3_INPUT_URI\": f\"{S3_OUTPUT_URI}/dataset\",\n",
    "    \"S3_OUTPUT_URI\": S3_OUTPUT_URI,\n",
    "}\n",
    "\n",
    "stage2_trainer = launch_stage(\"describe\", stage2_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 2 to complete\n",
    "stage2_result = wait_for_job(stage2_trainer)\n",
    "print(f\"Describe stage completed: {stage2_result['TrainingJobStatus']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display samples after Describe\n",
    "ds_describe = load_dataset_from_s3(f\"{S3_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_describe, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Assemble\n",
    "\n",
    "Enrich markdown with figure captions to create the final dataset.\n",
    "Input is read from S3 (output of Stage 2), output is saved to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Assemble\n",
    "# Updates dataset in place + saves final markdown files\n",
    "stage3_env = {\n",
    "    # Local output directory\n",
    "    \"OUTPUT_DIR\": \"./outputs\",\n",
    "    \n",
    "    # S3 input and output (same location - updates in place)\n",
    "    \"S3_INPUT_URI\": f\"{S3_OUTPUT_URI}/dataset\",\n",
    "    \"S3_OUTPUT_URI\": S3_OUTPUT_URI,\n",
    "    \n",
    "    # Assemble stage doesn't need GPU\n",
    "    \"SKIP_SERVER_LAUNCH\": \"true\",\n",
    "}\n",
    "\n",
    "stage3_trainer = launch_stage(\"assemble\", stage3_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for Stage 3 to complete\n",
    "stage3_result = wait_for_job(stage3_trainer)\n",
    "print(f\"Assemble stage completed: {stage3_result['TrainingJobStatus']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display final samples after Assemble\n",
    "ds_final = load_dataset_from_s3(f\"{S3_OUTPUT_URI}/dataset\")\n",
    "display_samples(ds_final, num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete!\n",
    "\n",
    "The OCR pipeline has finished. Your dataset is available in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nS3 Output Location: {S3_OUTPUT_URI}\")\n",
    "print(f\"  - Dataset: {S3_OUTPUT_URI}/dataset/\")\n",
    "print(f\"  - Files: {S3_OUTPUT_URI}/outputs/\")\n",
    "print(f\"\\nS3 Job Output: s3://{BUCKET_NAME}/{S3_PREFIX}/output/\")\n",
    "print(\"\\nJob Summary:\")\n",
    "for i, (name, result) in enumerate([\n",
    "    (\"Extract\", stage1_result),\n",
    "    (\"Describe\", stage2_result),\n",
    "    (\"Assemble\", stage3_result),\n",
    "], 1):\n",
    "    status = result[\"TrainingJobStatus\"]\n",
    "    print(f\"  {i}. {name}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Final Dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final assembled dataset from S3\n",
    "from datasets import load_from_disk\n",
    "import tempfile\n",
    "\n",
    "# Download from S3\n",
    "s3 = boto3.client('s3')\n",
    "local_dataset_dir = Path(tempfile.mkdtemp(prefix=\"final-dataset-\"))\n",
    "\n",
    "# Parse S3 URI to get bucket and prefix\n",
    "s3_parts = S3_OUTPUT_URI.replace('s3://', '').split('/', 1)\n",
    "bucket = s3_parts[0]\n",
    "prefix = f\"{s3_parts[1]}/dataset/\" if len(s3_parts) > 1 else \"dataset/\"\n",
    "\n",
    "# List and download all files from the dataset\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "\n",
    "for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        rel_path = key[len(prefix):]\n",
    "        if rel_path:\n",
    "            local_path = local_dataset_dir / rel_path\n",
    "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            s3.download_file(bucket, key, str(local_path))\n",
    "\n",
    "# Load the dataset\n",
    "final_dataset = load_from_disk(str(local_dataset_dir))\n",
    "print(f\"Loaded {len(final_dataset)} samples from {S3_OUTPUT_URI}/dataset/\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary source directory\n",
    "try:\n",
    "    shutil.rmtree(source_dir)\n",
    "    print(f\"Cleaned up: {source_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not clean up: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
